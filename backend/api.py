#!/usr/bin/env python3
"""
Flask API for file processing and Gemini integration
"""

from flask import Flask, request, jsonify, send_file
from werkzeug.utils import secure_filename
import os
import tempfile
from pathlib import Path
import json
from typing import Dict, Any
from docx import Document as DocxDocument

# Import our CLI functions
from cli import extract_file_content, set_content_processor, example_processor

app = Flask(__name__)
app.config["MAX_CONTENT_LENGTH"] = 16 * 1024 * 1024  # 16MB max file size

# Configure upload folder
UPLOAD_FOLDER = "uploads"
ALLOWED_EXTENSIONS = {
    "txt",
    "json",
    "csv",
    "pdf",
    "docx",
    "xlsx",
    "xls",
    "ipc",
    "jpg",
    "jpeg",
    "png",
    "gif",
    "bmp",
    "tiff",
}

# Create upload directory if it doesn't exist
os.makedirs(UPLOAD_FOLDER, exist_ok=True)


def allowed_file(filename):
    """Check if file extension is allowed"""
    return "." in filename and filename.rsplit(".", 1)[1].lower() in ALLOWED_EXTENSIONS


def mock_gemini_call(text: str) -> str:
    """Mock Gemini API call - replace with actual Gemini integration later"""
    # This is a mock response that simulates what Gemini might return
    mock_responses = [
        f"Based on the provided content ({len(text)} characters), here's an analysis:\n\n"
        f"Key insights:\n"
        f"- The document contains {len(text.split())} words\n"
        f"- Main topics appear to be related to data processing\n"
        f"- Recommended actions: Review content structure and optimize for clarity\n\n"
        f"This is a mock response from Gemini API. In production, this would be replaced with actual AI processing.",
        f"Document Summary:\n\n"
        f"Content Analysis:\n"
        f"- Character count: {len(text)}\n"
        f"- Word count: {len(text.split())}\n"
        f"- Estimated reading time: {len(text.split()) // 200} minutes\n\n"
        f"Recommendations:\n"
        f"- Consider adding more structure to improve readability\n"
        f"- Review for consistency in formatting\n"
        f"- Ensure all key points are clearly articulated\n\n"
        f"Note: This is a mock Gemini response for testing purposes.",
        f"AI-Generated Analysis:\n\n"
        f"Processing Results:\n"
        f"- Input size: {len(text)} characters\n"
        f"- Content type: Mixed format document\n"
        f"- Processing status: Complete\n\n"
        f"Key Findings:\n"
        f"- Document appears to be well-structured\n"
        f"- Contains relevant information for processing\n"
        f"- Suitable for further analysis\n\n"
        f"Next Steps:\n"
        f"- Implement actual Gemini API integration\n"
        f"- Add more sophisticated content analysis\n"
        f"- Enhance output formatting\n\n"
        f"This response was generated by a mock Gemini service.",
    ]

    # Return a different mock response based on content length
    if len(text) < 100:
        return mock_responses[0]
    elif len(text) < 500:
        return mock_responses[1]
    else:
        return mock_responses[2]


def create_docx_from_text(
    text: str, filename: str = "output.docx", original_content: str = None
) -> str:
    """Create a DOCX file from text content"""
    try:
        from docx import Document

        # Create a new Document
        doc = Document()

        # Add title
        title = doc.add_heading("AI-Generated Document", 0)

        # Add original content section if provided
        if original_content:
            doc.add_heading("Original Input Content", level=1)
            doc.add_paragraph(
                "The following is the original content that was processed:"
            )
            doc.add_paragraph("─" * 50)

            # Split original content into paragraphs and add them
            original_paragraphs = original_content.split("\n")
            for para in original_paragraphs:
                if para.strip():
                    doc.add_paragraph(para.strip())

            doc.add_paragraph("─" * 50)
            doc.add_paragraph("")  # Add some spacing

        # Add processed content
        doc.add_heading("AI Analysis & Processing Results", level=1)

        # Split text into paragraphs and add them
        paragraphs = text.split("\n\n")
        for para in paragraphs:
            if para.strip():
                doc.add_paragraph(para.strip())

        # Add metadata
        doc.add_heading("Document Information", level=1)
        from datetime import datetime

        doc.add_paragraph(
            f'Generated on: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}'
        )
        doc.add_paragraph(f"Content length: {len(text)} characters")
        doc.add_paragraph(f"Word count: {len(text.split())} words")

        if original_content:
            doc.add_paragraph(
                f"Original content length: {len(original_content)} characters"
            )
            doc.add_paragraph(
                f"Original word count: {len(original_content.split())} words"
            )

        # Save to temporary file
        temp_dir = tempfile.gettempdir()
        output_path = os.path.join(temp_dir, filename)
        doc.save(output_path)

        return output_path

    except ImportError:
        raise Exception(
            "python-docx library not installed. Please install it with: pip install python-docx"
        )
    except Exception as e:
        raise Exception(f"Error creating DOCX file: {str(e)}")


def merge_docx_files(input_paths, output_filename: str) -> str:
    """Merge multiple DOCX files into a single DOCX, preserving formatting.

    Inserts an AI-generated heading at the top to indicate the file was
    generated/processed by the AI pipeline.
    Returns the path to the merged DOCX file.
    """
    try:
        # Create a new blank document and add AI-generated marker
        merged = DocxDocument()
        merged.add_heading("AI-Generated Document", level=1)
        merged.add_paragraph(
            "Note: This document contains content combined and processed by an AI-generated workflow."
        )

        # Append the body of each document into merged
        for path in input_paths:
            try:
                src = DocxDocument(path)
            except Exception as e:
                # skip unreadable docs but include an error paragraph
                merged.add_paragraph(
                    f"[Unable to include {os.path.basename(path)}: {e}]"
                )
                continue

            # Append each element from source body to merged body
            for element in src.element.body:
                merged.element.body.append(element)

        # Save merged document to temp file
        temp_dir = tempfile.gettempdir()
        out_path = os.path.join(temp_dir, output_filename)
        merged.save(out_path)
        return out_path
    except Exception as e:
        raise Exception(f"Error merging DOCX files: {e}")


@app.route("/health", methods=["GET"])
def health_check():
    """Health check endpoint"""
    return jsonify(
        {"status": "healthy", "message": "API is running", "version": "1.0.0"}
    )


@app.route("/upload", methods=["POST"])
def upload_file():
    """Upload and process files - similar to CLI extract command"""
    try:
        # Check if file is present
        if "file" not in request.files:
            return jsonify({"error": "No file provided"}), 400

        file = request.files["file"]

        if file.filename == "":
            return jsonify({"error": "No file selected"}), 400

        if not allowed_file(file.filename):
            return jsonify({"error": "File type not allowed"}), 400

        # Save uploaded file
        filename = secure_filename(file.filename)
        file_path = os.path.join(UPLOAD_FOLDER, filename)
        file.save(file_path)

        # Process file using CLI function
        result = extract_file_content(file_path)

        # Clean up uploaded file
        os.remove(file_path)

        # Return result
        return jsonify({"success": True, "filename": filename, "result": result})

    except Exception as e:
        return jsonify({"error": str(e)}), 500


@app.route("/process", methods=["POST"])
def process_files():
    """Process multiple files with optional custom processor"""
    try:
        data = request.get_json()

        if not data or "files" not in data:
            return jsonify({"error": "No files provided"}), 400

        files = data["files"]
        processor_name = data.get("processor", "default")

        if not isinstance(files, list):
            return jsonify({"error": "Files must be a list"}), 400

        # Set up processor if specified
        if processor_name != "default":
            set_content_processor(example_processor)

        results = []
        for file_path in files:
            if not os.path.exists(file_path):
                results.append({"file": file_path, "error": "File not found"})
                continue

            # Extract content
            content_result = extract_file_content(file_path)

            # Process content if processor is set
            if processor_name != "default" and "error" not in content_result:
                try:
                    processed = example_processor(
                        content_result["content"],
                        content_result["type"],
                        content_result["file"],
                        processor_name,
                    )
                    content_result["processed"] = processed
                except Exception as e:
                    content_result["processing_error"] = str(e)

            results.append(content_result)

        return jsonify(
            {"success": True, "results": results, "processor_used": processor_name}
        )

    except Exception as e:
        return jsonify({"error": str(e)}), 500


@app.route("/gemini", methods=["POST"])
def gemini_process():
    """Mock Gemini processing with DOCX output"""
    print("Starting Gemini processing")
    try:
        data = request.get_json()
        print("Data: ", data)

        if not data or "text" not in data:
            return jsonify({"error": "No text provided"}), 400

        text = data["text"]

        # Mock Gemini call
        gemini_response = mock_gemini_call(text)
        print("Gemini response: ", gemini_response)

        # Create DOCX file
        docx_filename = f"gemini_output_{os.urandom(4).hex()}.docx"
        print("Docx filename: ", docx_filename)
        docx_path = create_docx_from_text(gemini_response, docx_filename, text)
        print("Docx path: ", docx_path)

        # Return DOCX file
        return send_file(
            docx_path,
            as_attachment=True,
            download_name=docx_filename,
            mimetype="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
        )

    except Exception as e:
        return jsonify({"error": str(e)}), 500


@app.route("/gemini-text", methods=["POST"])
def gemini_text_only():
    """Mock Gemini processing - text only (no DOCX)"""
    try:
        data = request.get_json()

        if not data or "text" not in data:
            return jsonify({"error": "No text provided"}), 400

        text = data["text"]

        # Mock Gemini call
        gemini_response = mock_gemini_call(text)

        return jsonify(
            {
                "success": True,
                "original_text": text,
                "gemini_response": gemini_response,
                "response_length": len(gemini_response),
            }
        )

    except Exception as e:
        return jsonify({"error": str(e)}), 500


@app.route("/upload-and-gemini", methods=["POST"])
def upload_and_gemini():
    """Upload file, extract content, process with Gemini, and return DOCX"""
    try:
        # Check if file is present
        if "file" not in request.files:
            return jsonify({"error": "No file provided"}), 400

        file = request.files["file"]

        if file.filename == "":
            return jsonify({"error": "No file selected"}), 400

        if not allowed_file(file.filename):
            return jsonify({"error": "File type not allowed"}), 400

        # Save uploaded file
        filename = secure_filename(file.filename)
        file_path = os.path.join(UPLOAD_FOLDER, filename)
        file.save(file_path)

        # Extract content
        content_result = extract_file_content(file_path)

        # Clean up uploaded file
        os.remove(file_path)

        if "error" in content_result:
            return (
                jsonify(
                    {"error": f"File processing failed: {content_result['error']}"}
                ),
                500,
            )

        # Process with Gemini
        gemini_response = mock_gemini_call(content_result["content"])

        # Create DOCX file
        docx_filename = f"processed_{filename.rsplit('.', 1)[0]}_gemini.docx"
        docx_path = create_docx_from_text(
            gemini_response, docx_filename, content_result["content"]
        )

        # Return DOCX file
        return send_file(
            docx_path,
            as_attachment=True,
            download_name=docx_filename,
            mimetype="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
        )

    except Exception as e:
        return jsonify({"error": str(e)}), 500


@app.errorhandler(413)
def too_large(e):
    """Handle file too large error"""
    return jsonify({"error": "File too large. Maximum size is 16MB."}), 413


@app.errorhandler(404)
def not_found(e):
    """Handle 404 errors"""
    return jsonify({"error": "Endpoint not found"}), 404


@app.errorhandler(500)
def internal_error(e):
    """Handle 500 errors"""
    return jsonify({"error": "Internal server error"}), 500


@app.route("/upload-multiple", methods=["POST"])
def upload_multiple():
    """Accept multiple files and return list of file info and extracted results"""
    try:
        # Flask provides `request.files` as an ImmutableMultiDict; multiple files per key are supported
        if not request.files:
            return jsonify({"error": "No files provided"}), 400

        # Collect file info into a list for downstream processing
        responses = []

        for key in request.files:
            filelist = request.files.getlist(key)
            for file in filelist:
                item = {"field": key, "filename": file.filename}

                if file.filename == "":
                    item["error"] = "No filename provided"
                    responses.append(item)
                    continue

                if not allowed_file(file.filename):
                    item["error"] = "File type not allowed"
                    responses.append(item)
                    continue

                # Save to a temporary file for processing
                filename = secure_filename(file.filename)
                temp_dir = tempfile.gettempdir()
                tmp_path = os.path.join(
                    temp_dir, f"upload_multi_{os.urandom(6).hex()}_{filename}"
                )
                file.save(tmp_path)

                try:
                    # Use existing extract_file_content to detect type and extract content
                    result = extract_file_content(tmp_path)
                    # Keep only basic metadata to pass to processor
                    item["type"] = (
                        result.get("type") if isinstance(result, dict) else None
                    )
                    item["size"] = (
                        result.get("size") if isinstance(result, dict) else None
                    )
                    item["file"] = (
                        result.get("file") if isinstance(result, dict) else None
                    )
                    # Include extracted content when available so downstream
                    # processors can consume it directly without re-reading files.
                    item["content"] = (
                        result.get("content") if isinstance(result, dict) else None
                    )
                except Exception as e:
                    item["error"] = str(e)
                finally:
                    try:
                        os.remove(tmp_path)
                    except Exception:
                        pass

                responses.append(item)

        # Call a downstream processor that consumes the list of files.
        try:
            process_file_list(responses)
        except Exception as e:
            return jsonify({"error": f"Processing failed: {e}"}), 500

        # For now the processor just prints; return a simple 200 response
        return jsonify({"success": True, "message": "Files processed"}), 200

    except Exception as e:
        return jsonify({"error": str(e)}), 500


def process_file_list(file_list):
    """Simple downstream processor that prints filename and detected type.

    This is a placeholder to demonstrate consuming the list of file info.
    In production this could enqueue jobs, call another service, etc.
    """
    print("Processing file list with", len(file_list), "items")
    for f in file_list:
        fname = f.get("filename")
        ftype = f.get("type")
        print(f"  - {fname} (type: {ftype})")

        # If content is present, print a reasonable preview so it's visible
        # in logs. Truncate long contents to avoid huge log entries.
        content = f.get("content")
        if content:
            try:
                preview = (
                    content
                    if len(content) <= 1000
                    else content[:1000] + "... [truncated]"
                )
            except Exception:
                preview = "[binary or non-text content]"
            print(f"    Content preview: {preview}")
    print("File list processing complete.")


@app.route("/merge-docx", methods=["POST"])
def merge_docx_endpoint():
    """Upload multiple DOCX files and return a single merged DOCX."""
    try:
        if not request.files:
            return jsonify({"error": "No files provided"}), 400

        doc_paths = []
        for key in request.files:
            for f in request.files.getlist(key):
                if not f.filename.lower().endswith(".docx"):
                    continue
                # save to temp
                filename = secure_filename(f.filename)
                tmp_path = os.path.join(
                    tempfile.gettempdir(), f"merge_{os.urandom(6).hex()}_{filename}"
                )
                f.save(tmp_path)
                doc_paths.append(tmp_path)

        if not doc_paths:
            return jsonify({"error": "No DOCX files provided"}), 400

        out_name = f"merged_{os.urandom(4).hex()}.docx"
        merged_path = merge_docx_files(doc_paths, out_name)

        # cleanup input temp files
        for p in doc_paths:
            try:
                os.remove(p)
            except Exception:
                pass

        return send_file(
            merged_path,
            as_attachment=True,
            download_name=out_name,
            mimetype="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
        )
    except Exception as e:
        return jsonify({"error": str(e)}), 500


if __name__ == "__main__":
    # Set up the example processor
    set_content_processor(example_processor)

    print("Starting Flask API server...")
    print("Available endpoints:")
    print("  GET  /health - Health check")
    print("  POST /upload - Upload and process single file")
    print("  POST /process - Process multiple files with custom processor")
    print("  POST /gemini - Mock Gemini processing with DOCX output")
    print("  POST /gemini-text - Mock Gemini processing (text only)")
    print("  POST /upload-and-gemini - Upload file, process with Gemini, return DOCX")

    app.run(debug=True, host="0.0.0.0", port=5000)
