#!/usr/bin/env python3
"""
Flask API for file processing and Gemini integration
"""

from flask import Flask, request, jsonify, send_file
from werkzeug.utils import secure_filename
import os
import tempfile
from pathlib import Path
import json
from typing import Dict, Any
from docx import Document as DocxDocument
import copy


def _local_name(tag: str) -> str:
    return tag.split("}")[-1] if "}" in tag else tag


IMAGE_TAGS = {"drawing", "pict", "blip", "graphic", "graphicdata"}
COMMENT_TAGS = {"commentrangestart", "commentrangeend", "commentreference", "comment"}
REVISION_TAGS = {
    "ins",
    "del",
    "movefrom",
    "moveto",
    "movefromrangestart",
    "movefromrangeend",
    "movetorangeend",
    "revision",
}

# Import our CLI functions
from cli import extract_file_content, set_content_processor, example_processor

app = Flask(__name__)
app.config["MAX_CONTENT_LENGTH"] = 16 * 1024 * 1024  # 16MB max file size

# Configure upload folder
UPLOAD_FOLDER = "uploads"
ALLOWED_EXTENSIONS = {
    "txt",
    "json",
    "csv",
    "pdf",
    "docx",
    "xlsx",
    "xls",
    "ipc",
    "jpg",
    "jpeg",
    "png",
    "gif",
    "bmp",
    "tiff",
}

# Create upload directory if it doesn't exist
os.makedirs(UPLOAD_FOLDER, exist_ok=True)


def allowed_file(filename):
    """Check if file extension is allowed"""
    return "." in filename and filename.rsplit(".", 1)[1].lower() in ALLOWED_EXTENSIONS


def mock_gemini_call(text: str) -> str:
    """Mock Gemini API call - replace with actual Gemini integration later"""
    # This is a mock response that simulates what Gemini might return
    mock_responses = [
        f"Based on the provided content ({len(text)} characters), here's an analysis:\n\n"
        f"Key insights:\n"
        f"- The document contains {len(text.split())} words\n"
        f"- Main topics appear to be related to data processing\n"
        f"- Recommended actions: Review content structure and optimize for clarity\n\n"
        f"This is a mock response from Gemini API. In production, this would be replaced with actual AI processing.",
        f"Document Summary:\n\n"
        f"Content Analysis:\n"
        f"- Character count: {len(text)}\n"
        f"- Word count: {len(text.split())}\n"
        f"- Estimated reading time: {len(text.split()) // 200} minutes\n\n"
        f"Recommendations:\n"
        f"- Consider adding more structure to improve readability\n"
        f"- Review for consistency in formatting\n"
        f"- Ensure all key points are clearly articulated\n\n"
        f"Note: This is a mock Gemini response for testing purposes.",
        f"AI-Generated Analysis:\n\n"
        f"Processing Results:\n"
        f"- Input size: {len(text)} characters\n"
        f"- Content type: Mixed format document\n"
        f"- Processing status: Complete\n\n"
        f"Key Findings:\n"
        f"- Document appears to be well-structured\n"
        f"- Contains relevant information for processing\n"
        f"- Suitable for further analysis\n\n"
        f"Next Steps:\n"
        f"- Implement actual Gemini API integration\n"
        f"- Add more sophisticated content analysis\n"
        f"- Enhance output formatting\n\n"
        f"This response was generated by a mock Gemini service.",
    ]

    # Return a different mock response based on content length
    if len(text) < 100:
        return mock_responses[0]
    elif len(text) < 500:
        return mock_responses[1]
    else:
        return mock_responses[2]


def create_docx_from_text(
    text: str, filename: str = "output.docx", original_content: str = None
) -> str:
    """Create a DOCX file from text content"""
    try:
        from docx import Document

        # Create a new Document
        doc = Document()

        # Add title
        title = doc.add_heading("AI-Generated Document", 0)

        # Add original content section if provided
        if original_content:
            doc.add_heading("Original Input Content", level=1)
            doc.add_paragraph(
                "The following is the original content that was processed:"
            )
            doc.add_paragraph("─" * 50)

            # Split original content into paragraphs and add them
            original_paragraphs = original_content.split("\n")
            for para in original_paragraphs:
                if para.strip():
                    doc.add_paragraph(para.strip())

            doc.add_paragraph("─" * 50)
            doc.add_paragraph("")  # Add some spacing

        # Add processed content
        doc.add_heading("AI Analysis & Processing Results", level=1)

        # Split text into paragraphs and add them
        paragraphs = text.split("\n\n")
        for para in paragraphs:
            if para.strip():
                doc.add_paragraph(para.strip())

        # Add metadata
        doc.add_heading("Document Information", level=1)
        from datetime import datetime

        doc.add_paragraph(
            f'Generated on: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}'
        )
        doc.add_paragraph(f"Content length: {len(text)} characters")
        doc.add_paragraph(f"Word count: {len(text.split())} words")

        if original_content:
            doc.add_paragraph(
                f"Original content length: {len(original_content)} characters"
            )
            doc.add_paragraph(
                f"Original word count: {len(original_content.split())} words"
            )

        # Save to temporary file
        temp_dir = tempfile.gettempdir()
        output_path = os.path.join(temp_dir, filename)
        doc.save(output_path)

        return output_path

    except ImportError:
        raise Exception(
            "python-docx library not installed. Please install it with: pip install python-docx"
        )
    except Exception as e:
        raise Exception(f"Error creating DOCX file: {str(e)}")


def merge_docx_files(input_paths, output_filename: str) -> str:
    """Merge multiple DOCX files into a single DOCX, preserving formatting.

    Inserts an AI-generated heading at the top to indicate the file was
    generated/processed by the AI pipeline.
    Returns the path to the merged DOCX file.
    """
    try:
        # Create a new blank document and add AI-generated marker
        merged = DocxDocument()
        merged.add_heading("AI-Generated Document", level=1)
        merged.add_paragraph(
            "Note: This document contains content combined and processed by an AI-generated workflow."
        )

        # Append the content of each document into merged, preserving
        # paragraph/table order and basic run formatting. Images are
        # excluded because we only copy run.text.
        def _copy_run(src_run, dest_run):
            # Copy basic run-level formatting where available
            try:
                dest_run.bold = src_run.bold
                dest_run.italic = src_run.italic
                dest_run.underline = src_run.underline
                dest_run.strike = src_run.strike
            except Exception:
                pass
            try:
                # Font properties
                if src_run.font is not None and dest_run.font is not None:
                    if getattr(src_run.font, "name", None):
                        dest_run.font.name = src_run.font.name
                    if getattr(src_run.font, "size", None):
                        dest_run.font.size = src_run.font.size
                    if getattr(src_run.font.color, "rgb", None):
                        dest_run.font.color.rgb = src_run.font.color.rgb
            except Exception:
                pass

        def _copy_paragraph(src_p, dest_parent):
            # dest_parent is a Document or a _Cell; use add_paragraph on it
            try:
                dest_p = dest_parent.add_paragraph()
            except Exception:
                # If dest_parent already has paragraphs (e.g., a cell), create one differently
                dest_p = dest_parent.paragraphs[0]

            # copy paragraph-level style if possible
            try:
                if src_p.style is not None:
                    dest_p.style = src_p.style
            except Exception:
                pass
            try:
                dest_p.alignment = src_p.alignment
            except Exception:
                pass

            for src_run in src_p.runs:
                text = src_run.text or ""
                if not text:
                    # No textual content in this run (likely an image-only run)
                    continue
                new_run = dest_p.add_run(text)
                _copy_run(src_run, new_run)

            # If paragraph appears empty (python-docx couldn't extract text),
            # attempt an XML fallback to capture field results (TOC entries, etc.).
            if not dest_p.text.strip():
                try:
                    # Concatenate text from all descendant text nodes
                    xml_text_parts = []
                    for node in src_p._p.iter():
                        # text content can be on .text or .tail
                        if getattr(node, "text", None):
                            xml_text_parts.append(str(node.text))
                        if getattr(node, "tail", None):
                            xml_text_parts.append(str(node.tail))
                    fallback_text = "".join(
                        [t for t in xml_text_parts if t and t.strip()]
                    ).strip()
                    if fallback_text:
                        # replace the empty paragraph content with fallback
                        dest_p.clear()
                        dest_p.add_run(fallback_text)
                except Exception:
                    pass

            return dest_p

        def _copy_table(src_tbl, dest_doc):
            # create destination table with same dimensions
            rows = len(src_tbl.rows)
            cols = len(src_tbl.rows[0].cells) if rows > 0 else 0
            if rows == 0 or cols == 0:
                return
            dest_tbl = dest_doc.add_table(rows=rows, cols=cols)
            try:
                dest_tbl.style = src_tbl.style
            except Exception:
                pass

            for r_idx, src_row in enumerate(src_tbl.rows):
                for c_idx, src_cell in enumerate(src_row.cells):
                    dest_cell = dest_tbl.rows[r_idx].cells[c_idx]
                    # Clear any default paragraph
                    for p in list(dest_cell.paragraphs):
                        p.clear()
                    # Copy paragraphs inside the cell
                    for src_p in src_cell.paragraphs:
                        _copy_paragraph(src_p, dest_cell)

        for path in input_paths:
            try:
                src = DocxDocument(path)
            except Exception as e:
                merged.add_paragraph(
                    f"[Unable to include {os.path.basename(path)}: {e}]"
                )
                continue

            # Add a subheading with the source filename
            merged.add_heading(os.path.basename(path), level=2)

            # Track TOC/fallback texts to avoid duplication
            seen_toc_texts = set()

            # To preserve original order, iterate over the XML body and use
            # paragraph/table counters to map to python-docx objects.
            p_idx = 0
            t_idx = 0
            for child in src.element.body:
                lname = _local_name(child.tag).lower()
                if lname == "p":
                    if p_idx < len(src.paragraphs):
                        src_p = src.paragraphs[p_idx]

                        # Detect TOC/field paragraphs by inspecting XML for field nodes
                        is_field = False
                        try:
                            for node in src_p._p.iter():
                                tag_local = _local_name(node.tag).lower()
                                if tag_local in ("fldsimple", "fldchar", "instrtext"):
                                    is_field = True
                                    break
                        except Exception:
                            is_field = False

                        if is_field:
                            # Extract only visible text pieces from field paragraphs.
                            # Word stores the visible text in <w:t> nodes, and page
                            # number/tab markers in <w:tab>. The <w:instrText> and
                            # <w:fldChar> nodes contain field codes/instructions and
                            # should be ignored to avoid raw field code leakage.
                            visible_parts = []
                            try:
                                for node in src_p._p.iter():
                                    tag_local = _local_name(node.tag).lower()
                                    # include text nodes and tail text
                                    if tag_local == "t" and getattr(node, "text", None):
                                        visible_parts.append(str(node.text))
                                    elif tag_local == "tab":
                                        # use a single tab/space to separate columns
                                        visible_parts.append("\t")
                                    elif getattr(node, "tail", None):
                                        # tail text can contain visible fragments
                                        visible_parts.append(str(node.tail))
                            except Exception:
                                pass

                            # Join and normalize whitespace
                            fallback_text = "".join(
                                [p for p in visible_parts if p and p.strip()]
                            ).strip()
                            if fallback_text and fallback_text not in seen_toc_texts:
                                seen_toc_texts.add(fallback_text)
                                try:
                                    merged.add_paragraph(fallback_text)
                                except Exception:
                                    _copy_paragraph(src_p, merged)
                            # else: skip duplicate or empty TOC entry
                        else:
                            _copy_paragraph(src_p, merged)
                    p_idx += 1
                elif lname == "tbl":
                    if t_idx < len(src.tables):
                        src_tbl = src.tables[t_idx]
                        _copy_table(src_tbl, merged)
                    t_idx += 1

            # Separator
            merged.add_paragraph("")

        # Save merged document to temp file
        temp_dir = tempfile.gettempdir()
        out_path = os.path.join(temp_dir, output_filename)
        merged.save(out_path)
        return out_path
    except Exception as e:
        raise Exception(f"Error merging DOCX files: {e}")


@app.route("/health", methods=["GET"])
def health_check():
    """Health check endpoint"""
    return jsonify(
        {"status": "healthy", "message": "API is running", "version": "1.0.0"}
    )


@app.route("/upload", methods=["POST"])
def upload_file():
    """Upload and process files - similar to CLI extract command"""
    try:
        # Check if file is present
        if "file" not in request.files:
            return jsonify({"error": "No file provided"}), 400

        file = request.files["file"]

        if file.filename == "":
            return jsonify({"error": "No file selected"}), 400

        if not allowed_file(file.filename):
            return jsonify({"error": "File type not allowed"}), 400

        # Save uploaded file
        filename = secure_filename(file.filename)
        file_path = os.path.join(UPLOAD_FOLDER, filename)
        file.save(file_path)

        # Process file using CLI function
        result = extract_file_content(file_path)

        # Clean up uploaded file
        os.remove(file_path)

        # Return result
        return jsonify({"success": True, "filename": filename, "result": result})

    except Exception as e:
        return jsonify({"error": str(e)}), 500


@app.route("/process", methods=["POST"])
def process_files():
    """Process multiple files with optional custom processor"""
    try:
        data = request.get_json()

        if not data or "files" not in data:
            return jsonify({"error": "No files provided"}), 400

        files = data["files"]
        processor_name = data.get("processor", "default")

        if not isinstance(files, list):
            return jsonify({"error": "Files must be a list"}), 400

        # Set up processor if specified
        if processor_name != "default":
            set_content_processor(example_processor)

        results = []
        for file_path in files:
            if not os.path.exists(file_path):
                results.append({"file": file_path, "error": "File not found"})
                continue

            # Extract content
            content_result = extract_file_content(file_path)

            # Process content if processor is set
            if processor_name != "default" and "error" not in content_result:
                try:
                    processed = example_processor(
                        content_result["content"],
                        content_result["type"],
                        content_result["file"],
                        processor_name,
                    )
                    content_result["processed"] = processed
                except Exception as e:
                    content_result["processing_error"] = str(e)

            results.append(content_result)

        return jsonify(
            {"success": True, "results": results, "processor_used": processor_name}
        )

    except Exception as e:
        return jsonify({"error": str(e)}), 500


@app.route("/gemini", methods=["POST"])
def gemini_process():
    """Mock Gemini processing with DOCX output"""
    print("Starting Gemini processing")
    try:
        data = request.get_json()
        print("Data: ", data)

        if not data or "text" not in data:
            return jsonify({"error": "No text provided"}), 400

        text = data["text"]

        # Mock Gemini call
        gemini_response = mock_gemini_call(text)
        print("Gemini response: ", gemini_response)

        # Create DOCX file
        docx_filename = f"gemini_output_{os.urandom(4).hex()}.docx"
        print("Docx filename: ", docx_filename)
        docx_path = create_docx_from_text(gemini_response, docx_filename, text)
        print("Docx path: ", docx_path)

        # Return DOCX file
        return send_file(
            docx_path,
            as_attachment=True,
            download_name=docx_filename,
            mimetype="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
        )

    except Exception as e:
        return jsonify({"error": str(e)}), 500


@app.route("/gemini-text", methods=["POST"])
def gemini_text_only():
    """Mock Gemini processing - text only (no DOCX)"""
    try:
        data = request.get_json()

        if not data or "text" not in data:
            return jsonify({"error": "No text provided"}), 400

        text = data["text"]

        # Mock Gemini call
        gemini_response = mock_gemini_call(text)

        return jsonify(
            {
                "success": True,
                "original_text": text,
                "gemini_response": gemini_response,
                "response_length": len(gemini_response),
            }
        )

    except Exception as e:
        return jsonify({"error": str(e)}), 500


@app.route("/upload-and-gemini", methods=["POST"])
def upload_and_gemini():
    """Upload file, extract content, process with Gemini, and return DOCX"""
    try:
        # Check if file is present
        if "file" not in request.files:
            return jsonify({"error": "No file provided"}), 400

        file = request.files["file"]

        if file.filename == "":
            return jsonify({"error": "No file selected"}), 400

        if not allowed_file(file.filename):
            return jsonify({"error": "File type not allowed"}), 400

        # Save uploaded file
        filename = secure_filename(file.filename)
        file_path = os.path.join(UPLOAD_FOLDER, filename)
        file.save(file_path)

        # Extract content
        content_result = extract_file_content(file_path)

        # Clean up uploaded file
        os.remove(file_path)

        if "error" in content_result:
            return (
                jsonify(
                    {"error": f"File processing failed: {content_result['error']}"}
                ),
                500,
            )

        # Process with Gemini
        gemini_response = mock_gemini_call(content_result["content"])

        # Create DOCX file
        docx_filename = f"processed_{filename.rsplit('.', 1)[0]}_gemini.docx"
        docx_path = create_docx_from_text(
            gemini_response, docx_filename, content_result["content"]
        )

        # Return DOCX file
        return send_file(
            docx_path,
            as_attachment=True,
            download_name=docx_filename,
            mimetype="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
        )

    except Exception as e:
        return jsonify({"error": str(e)}), 500


@app.errorhandler(413)
def too_large(e):
    """Handle file too large error"""
    return jsonify({"error": "File too large. Maximum size is 16MB."}), 413


@app.errorhandler(404)
def not_found(e):
    """Handle 404 errors"""
    return jsonify({"error": "Endpoint not found"}), 404


@app.errorhandler(500)
def internal_error(e):
    """Handle 500 errors"""
    return jsonify({"error": "Internal server error"}), 500


@app.route("/upload-multiple", methods=["POST"])
def upload_multiple():
    """Accept multiple files and return list of file info and extracted results"""
    try:
        # Flask provides `request.files` as an ImmutableMultiDict; multiple files per key are supported
        if not request.files:
            return jsonify({"error": "No files provided"}), 400

        # Collect file info into a list for downstream processing
        responses = []

        for key in request.files:
            filelist = request.files.getlist(key)
            for file in filelist:
                item = {"field": key, "filename": file.filename}

                if file.filename == "":
                    item["error"] = "No filename provided"
                    responses.append(item)
                    continue

                if not allowed_file(file.filename):
                    item["error"] = "File type not allowed"
                    responses.append(item)
                    continue

                # Save to a temporary file for processing
                filename = secure_filename(file.filename)
                temp_dir = tempfile.gettempdir()
                tmp_path = os.path.join(
                    temp_dir, f"upload_multi_{os.urandom(6).hex()}_{filename}"
                )
                file.save(tmp_path)

                try:
                    # Use existing extract_file_content to detect type and extract content
                    result = extract_file_content(tmp_path)
                    # Keep only basic metadata to pass to processor
                    item["type"] = (
                        result.get("type") if isinstance(result, dict) else None
                    )
                    item["size"] = (
                        result.get("size") if isinstance(result, dict) else None
                    )
                    item["file"] = (
                        result.get("file") if isinstance(result, dict) else None
                    )
                    # Include extracted content when available so downstream
                    # processors can consume it directly without re-reading files.
                    item["content"] = (
                        result.get("content") if isinstance(result, dict) else None
                    )
                except Exception as e:
                    item["error"] = str(e)
                finally:
                    try:
                        os.remove(tmp_path)
                    except Exception:
                        pass

                responses.append(item)

        # Call a downstream processor that consumes the list of files.
        try:

            # 

            process_file_list(responses)
        except Exception as e:
            return jsonify({"error": f"Processing failed: {e}"}), 500

        # For now the processor just prints; return a simple 200 response
        return jsonify({"success": True, "message": "Files processed"}), 200

    except Exception as e:
        return jsonify({"error": str(e)}), 500


def process_file_list(file_list):
    """Simple downstream processor that prints filename and detected type.

    This is a placeholder to demonstrate consuming the list of file info.
    In production this could enqueue jobs, call another service, etc.
    """
    print("Processing file list with", len(file_list), "items")
    for f in file_list:
        fname = f.get("filename")
        ftype = f.get("type")
        print(f"  - {fname} (type: {ftype})")

        # If content is present, print a reasonable preview so it's visible
        # in logs. Truncate long contents to avoid huge log entries.
        content = f.get("content")
        if content:
            try:
                preview = (
                    content
                    if len(content) <= 1000
                    else content[:1000] + "... [truncated]"
                )
            except Exception:
                preview = "[binary or non-text content]"
            print(f"    Content preview: {preview}")
    print("File list processing complete.")


@app.route("/merge-docx", methods=["POST"])
def merge_docx_endpoint():
    """Upload multiple DOCX files and return a single merged DOCX."""
    try:
        if not request.files:
            return jsonify({"error": "No files provided"}), 400

        doc_paths = []
        for key in request.files:
            for f in request.files.getlist(key):
                if not f.filename.lower().endswith(".docx"):
                    continue
                # save to temp
                filename = secure_filename(f.filename)
                tmp_path = os.path.join(
                    tempfile.gettempdir(), f"merge_{os.urandom(6).hex()}_{filename}"
                )
                f.save(tmp_path)
                doc_paths.append(tmp_path)

        if not doc_paths:
            return jsonify({"error": "No DOCX files provided"}), 400

        out_name = f"merged_{os.urandom(4).hex()}.docx"
        merged_path = merge_docx_files(doc_paths, out_name)

        # cleanup input temp files
        for p in doc_paths:
            try:
                os.remove(p)
            except Exception:
                pass

        return send_file(
            merged_path,
            as_attachment=True,
            download_name=out_name,
            mimetype="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
        )
    except Exception as e:
        return jsonify({"error": str(e)}), 500


if __name__ == "__main__":
    # Set up the example processor
    set_content_processor(example_processor)

    print("Starting Flask API server...")
    print("Available endpoints:")
    print("  GET  /health - Health check")
    print("  POST /upload - Upload and process single file")
    print("  POST /process - Process multiple files with custom processor")
    print("  POST /gemini - Mock Gemini processing with DOCX output")
    print("  POST /gemini-text - Mock Gemini processing (text only)")
    print("  POST /upload-and-gemini - Upload file, process with Gemini, return DOCX")

    app.run(debug=True, host="0.0.0.0", port=5000)
